#!/usr/bin/env python3
"""
Quick Start Example for UCAN: Towards Strong Certified Defense with Asymmetric Randomization

This script provides a minimal working example of how to use UCAN for certified adversarial robustness.
It demonstrates the three main NPG methods: pattern-wise, dataset-wise, and certification-wise.
"""

import os
import sys
import torch
import torch.nn as nn
import numpy as np
from pathlib import Path

# Add parent directory to path to import UCAN modules
sys.path.append(str(Path(__file__).parent.parent))

try:
    from datasets import get_dataset
    from architectures import get_architecture
    from noisegenerator import NoiseGenerator
    from noises import GaussianNoise
    from core import Smooth
except ImportError as e:
    print(f"Import error: {e}")
    print("Please make sure you're running this from the UCAN directory")
    print("and that all dependencies are installed (pip install -r requirements.txt)")
    sys.exit(1)


def quick_demo():
    """
    Quick demonstration of UCAN's core functionality
    """
    print("üöÄ UCAN Quick Start Demo")
    print("=" * 50)
    
    # Set up device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")
    
    # Load a small dataset for demo (MNIST)
    print("\nüìä Loading MNIST dataset...")
    try:
        dataset = get_dataset("mnist", "test")
        print(f"Dataset loaded: {len(dataset)} samples")
    except Exception as e:
        print(f"Could not load dataset: {e}")
        print("This is expected if dataset files are not available.")
        print("The demo will continue with synthetic data.")
        dataset = None
    
    # Load or create a simple model
    print("\nüß† Setting up model...")
    try:
        model = get_architecture("mnist_cnn", "mnist")
        model.eval()
        print("Model architecture loaded successfully")
    except Exception as e:
        print(f"Could not load model: {e}")
        print("Using a simple synthetic model for demonstration")
        # Create a simple model for demo
        model = nn.Sequential(
            nn.Flatten(),
            nn.Linear(784, 128),
            nn.ReLU(),
            nn.Linear(128, 10)
        )
        model.eval()
    
    # Create synthetic input for demo
    print("\nüéØ Creating synthetic input...")
    x = torch.randn(1, 1, 28, 28)  # Single MNIST-like image
    print(f"Input shape: {x.shape}")
    
    # Demo 1: Pattern-wise Anisotropic Noise
    print("\n1Ô∏è‚É£ Pattern-wise Anisotropic Noise (Low Optimality)")
    print("-" * 45)
    
    # Define a simple center-focus pattern
    def center_pattern(i, j, height=28, width=28):
        """Simple pattern with lower noise in center"""
        center_i, center_j = height // 2, width // 2
        distance = np.sqrt((i - center_i)**2 + (j - center_j)**2)
        max_distance = np.sqrt(center_i**2 + center_j**2)
        return 0.1 + 0.9 * (distance / max_distance)
    
    # Create pattern-wise noise parameters
    sigma_pattern = torch.ones(1, 28, 28)
    for i in range(28):
        for j in range(28):
            sigma_pattern[0, i, j] = center_pattern(i, j)
    
    mu_pattern = torch.zeros(1, 28, 28)
    
    print(f"Pattern noise - min œÉ: {sigma_pattern.min():.3f}, max œÉ: {sigma_pattern.max():.3f}")
    
    # Demo 2: Dataset-wise Anisotropic Noise
    print("\n2Ô∏è‚É£ Dataset-wise Anisotropic Noise (Moderate Optimality)")
    print("-" * 50)
    
    # Simulate learned dataset-wise parameters
    sigma_dataset = torch.rand(1, 28, 28) * 0.8 + 0.2  # Random but reasonable values
    mu_dataset = torch.randn(1, 28, 28) * 0.1  # Small mean offsets
    
    print(f"Dataset noise - min œÉ: {sigma_dataset.min():.3f}, max œÉ: {sigma_dataset.max():.3f}")
    print(f"Dataset noise - mean Œº range: [{mu_dataset.min():.3f}, {mu_dataset.max():.3f}]")
    
    # Demo 3: Certification-wise Anisotropic Noise
    print("\n3Ô∏è‚É£ Certification-wise Anisotropic Noise (High Optimality)")
    print("-" * 52)
    
    # Simulate input-specific parameters (would normally be generated by NPG)
    sigma_cert = torch.rand(1, 28, 28) * 0.5 + 0.3  # Input-specific values
    mu_cert = torch.randn(1, 28, 28) * 0.2  # Larger mean adaptation
    
    print(f"Certification noise - min œÉ: {sigma_cert.min():.3f}, max œÉ: {sigma_cert.max():.3f}")
    print(f"Certification noise - mean Œº range: [{mu_cert.min():.3f}, {mu_cert.max():.3f}]")
    
    # Demo: Generate noise samples
    print("\nüîä Generating noise samples...")
    
    # Isotropic baseline (Cohen et al.)
    noise_iso = torch.randn_like(x) * 1.0
    x_noisy_iso = x + noise_iso
    
    # Anisotropic noise examples
    noise_pattern = torch.randn_like(x) * sigma_pattern + mu_pattern
    x_noisy_pattern = x + noise_pattern
    
    noise_cert = torch.randn_like(x) * sigma_cert + mu_cert
    x_noisy_cert = x + noise_cert
    
    # Compare noise characteristics
    print(f"Isotropic noise std: {noise_iso.std():.3f}")
    print(f"Pattern noise std: {noise_pattern.std():.3f}")
    print(f"Certification noise std: {noise_cert.std():.3f}")
    
    # Demo: Model predictions (if possible)
    print("\nüîÆ Model predictions...")
    try:
        with torch.no_grad():
            pred_clean = model(x)
            pred_iso = model(x_noisy_iso)
            pred_pattern = model(x_noisy_pattern)
            pred_cert = model(x_noisy_cert)
        
        print(f"Clean prediction entropy: {-torch.sum(torch.softmax(pred_clean, dim=1) * torch.log_softmax(pred_clean, dim=1)):.3f}")
        print(f"Isotropic noise entropy: {-torch.sum(torch.softmax(pred_iso, dim=1) * torch.log_softmax(pred_iso, dim=1)):.3f}")
        print(f"Pattern noise entropy: {-torch.sum(torch.softmax(pred_pattern, dim=1) * torch.log_softmax(pred_pattern, dim=1)):.3f}")
        print(f"Certification noise entropy: {-torch.sum(torch.softmax(pred_cert, dim=1) * torch.log_softmax(pred_cert, dim=1)):.3f}")
        
    except Exception as e:
        print(f"Could not run model predictions: {e}")
    
    # Summary
    print("\nüìä Summary")
    print("=" * 50)
    print("‚úÖ Successfully demonstrated UCAN's three NPG methods:")
    print("   ‚Ä¢ Pattern-wise: Fixed spatial patterns (efficient)")
    print("   ‚Ä¢ Dataset-wise: Learned dataset parameters (balanced)")
    print("   ‚Ä¢ Certification-wise: Input-specific parameters (optimal)")
    print("\nüéØ Next steps:")
    print("   ‚Ä¢ Run full training: python train_certification_noise.py ...")
    print("   ‚Ä¢ Perform certification: python certification_*.py ...")
    print("   ‚Ä¢ Check examples/demo.ipynb for interactive exploration")
    

if __name__ == "__main__":
    quick_demo()